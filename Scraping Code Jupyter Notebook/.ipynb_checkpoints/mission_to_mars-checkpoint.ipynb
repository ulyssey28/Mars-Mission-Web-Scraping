{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from splinter import Browser\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "executable_path = {'executable_path': 'chromedriver'}\n",
    "browser = Browser('chrome', **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mars News (NASA Website)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of NASA page to be scraped\n",
    "url = 'https://mars.nasa.gov/news/'\n",
    "\n",
    "# using the .visit() method tell the browser to visit the url\n",
    "browser.visit(url)\n",
    "\n",
    "#sleep code to allow browser page to load\n",
    "time.sleep(5)\n",
    "\n",
    "#obtain the page html using the .html attribute of the browser object\n",
    "html = browser.html\n",
    "\n",
    "# Create BeautifulSoup object; parse with 'html.parser'\n",
    "soup = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Page HTML\n",
    "# print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Collect the latest News Title and Paragraph Text. \n",
    "#Note the find method gets the first \"div\" tag with class = \"content_title\", corresponding to the latest news title \n",
    "news_title_tag =  soup.find(\"div\", class_=\"content_title\")\n",
    "news_title = news_title_tag.text\n",
    "\n",
    "news_paragraph_tag = soup.find(\"div\", class_=\"article_teaser_body\")\n",
    "news_paragraph = news_paragraph_tag.text\n",
    "\n",
    "\n",
    "#Collect full news article link\n",
    "news_path = news_title_tag.a[\"href\"]\n",
    "news_link = \"https://mars.nasa.gov\"+ news_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JPL Mars Space Images - Featured Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scrape latest featured image from Mars from NASA website\n",
    "img_url = 'https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars'\n",
    "\n",
    "# using the .visit() method tell the browser to visit the url\n",
    "browser.visit(img_url)\n",
    "\n",
    "#sleep code to allow browser page to load\n",
    "time.sleep(5)\n",
    "\n",
    "#obtain the page html using the .html attribute of the browser object\n",
    "featured_img_html = browser.html\n",
    "\n",
    "# Create BeautifulSoup object; parse with 'html.parser'\n",
    "img_soup = BeautifulSoup(featured_img_html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the <a> tag with id \"full_image\".. which contains a link to the featured image\n",
    "a_tag = img_soup.find(\"a\", id=\"full_image\")\n",
    "featured_img_link = a_tag[\"data-fancybox-href\"]\n",
    "\n",
    "# The current string is: (/spaceimages/images/mediumsize/PIA17357_ip.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "lg_featured_img_url = \"https://www.jpl.nasa.gov\" + featured_img_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.jpl.nasa.gov/spaceimages/images/mediumsize/PIA19980_ip.jpg'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lg_featured_img_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"This image, taken by NASA's Dawn spacecraft, shows a portion of the southern hemisphere of dwarf planet Ceres from an altitude of 915 miles (1,470 kilometers). The image was taken on Sept. 20, 2015, and has a resolution of 450 feet (140 meters) per pixel.\""
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instruct the browser to click the text connected to our a_tag by using the browser .click_link_by_partial_text() method\n",
    "# Note this takes us to a pop-up screen where we can find a link to a full article on the image\n",
    "browser.click_link_by_partial_text(a_tag.text.strip())\n",
    "\n",
    "# sleep to allow browser time to load\n",
    "time.sleep(3)\n",
    "\n",
    "browser.click_link_by_partial_text(\"more info\")\n",
    "\n",
    "time.sleep(3)\n",
    "# Grab the html of the image article page\n",
    "featured_info_html = browser.html\n",
    "\n",
    "# Create a Beautiful soup object using the parsed html\n",
    "featured_info_soup = BeautifulSoup(featured_info_html, 'html.parser')\n",
    "\n",
    "# Find the \"h1\" tag with the class article_title which contains the title of the image\n",
    "h1_article_title_tag = featured_info_soup.find(\"h1\", class_=\"article_title\")\n",
    "\n",
    "# Assign the title text to the variable featured_des\n",
    "featured_des = h1_article_title_tag.text.strip()\n",
    "\n",
    "# Find the \"div\" that contains the page paragraph content\n",
    "paragraphs_div = featured_info_soup.find(\"div\", class_=\"wysiwyg_content\")\n",
    "\n",
    "\n",
    "featured_text = paragraphs_div.p.text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'paragraphs_div' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-7f0e4510829c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mparagraphs_div\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'paragraphs_div' is not defined"
     ]
    }
   ],
   "source": [
    "paragraphs_div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(h1_article_title_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mars Weather (Twitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scrape the latest Mars weather tweet from Mars Weather page\n",
    "twitter_url = \"https://twitter.com/marswxreport?lang=en\"\n",
    "\n",
    "# using the .visit() method tell the browser to visit the url\n",
    "browser.visit(twitter_url)\n",
    "\n",
    "#sleep code to allow browser page to load\n",
    "time.sleep(5)\n",
    "\n",
    "#obtain the page html using the .html attribute of the browser object\n",
    "twitter_html = browser.html\n",
    "\n",
    "# Create BeautifulSoup object; parse with 'html.parser'\n",
    "twitter_soup = BeautifulSoup(twitter_html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine twitter page html using inspector and find the div class which refers to the container for each tweet\n",
    "tweet_container = twitter_soup.find(\"div\", class_=\"tweet\")\n",
    "\n",
    "# Grab the text within the first paragraph of the container\n",
    "mars_weather = tweet_container.p.text\n",
    "\n",
    "# Replace the newline symbols within the string with spaces\n",
    "mars_weather = mars_weather.replace(\"\\n\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mars Facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the read_html function in Pandas to automatically scrape any tabular data from a page.\n",
    "Marsfact_url = \"https://space-facts.com/mars/\"\n",
    "\n",
    "#using the pandas .read_html() method, we can convert the html tables found the page to pandas dataframes.\n",
    "# Note the result of the .read_html() method is a list of dataframes pulled from the page.. In this case there is only one table\n",
    "url_tables = pd.read_html(Marsfact_url, encoding= \"utf-8\")\n",
    "\n",
    "# Reference the first element of our list of dataframes\n",
    "mars_facts_df = url_tables[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Rename Columns and set index\n",
    "mars_facts_df = mars_facts_df.rename(columns={0: \"Description\"})\n",
    "mars_facts_df = mars_facts_df.rename(columns={1: \"Values\"})\n",
    "mars_facts_df = mars_facts_df.set_index(\"Description\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<table border=\"1\" class=\"dataframe\">  <thead>    <tr style=\"text-align: right;\">      <th></th>      <th>Values</th>    </tr>    <tr>      <th>Description</th>      <th></th>    </tr>  </thead>  <tbody>    <tr>      <th>Equatorial Diameter:</th>      <td>6,792 km</td>    </tr>    <tr>      <th>Polar Diameter:</th>      <td>6,752 km</td>    </tr>    <tr>      <th>Mass:</th>      <td>6.42 x 10^23 kg (10.7% Earth)</td>    </tr>    <tr>      <th>Moons:</th>      <td>2 (Phobos &amp; Deimos)</td>    </tr>    <tr>      <th>Orbit Distance:</th>      <td>227,943,824 km (1.52 AU)</td>    </tr>    <tr>      <th>Orbit Period:</th>      <td>687 days (1.9 years)</td>    </tr>    <tr>      <th>Surface Temperature:</th>      <td>-153 to 20 Â°C</td>    </tr>    <tr>      <th>First Record:</th>      <td>2nd millennium BC</td>    </tr>    <tr>      <th>Recorded By:</th>      <td>Egyptian astronomers</td>    </tr>  </tbody></table>'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mars_facts_df.to_html().replace('\\n', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mars Hemispheres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain high resolution images for each of Mar's hemispheres.\n",
    "hem_imgs_url = \"https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars\"\n",
    "\n",
    "# using the .visit() method tell the browser to visit the url\n",
    "browser.visit(hem_imgs_url)\n",
    "\n",
    "#sleep code to allow browser page to load\n",
    "time.sleep(5)\n",
    "\n",
    "#obtain the page html using the .html attribute of the browser object\n",
    "hem_html = browser.html\n",
    "# Create BeautifulSoup object; parse with 'html.parser'\n",
    "hem_soup = BeautifulSoup(hem_html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list that will hold our image info dictionaries\n",
    "hemisphere_image_info = []\n",
    "\n",
    "# Create a list of \"div\"s of class=\"description\"... which hold the information pertaining to our hemisphere titles\n",
    "soup_div_list = hem_soup.find_all(\"div\", class_=\"description\")\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" The following code loops through our list of \"divs\" of class= \"description\".\n",
    "For each \"div\":\n",
    "1. We first make sure we are on the starting hem_imgs_url page \n",
    "as we will be moving to several pages throughout the loop.\n",
    "\n",
    "2. We then time out our code as a precautionary measure since transitioning from page to page \n",
    "may require the browser to load at times\n",
    "\n",
    "3. We set the hem_title (hemisphere title) equal to the \"h3\" text found inside the current \"div\"\n",
    "\n",
    "4. We then transition to a new page with the splinter - click_link_by_partial_text() method using the hem_title as the argument\n",
    "since the link to the image page is connected to our hem_title text\n",
    "\n",
    "5. From there we obtain the html of the image page using the browser.html method\n",
    "\n",
    "6. We then create a soup object using the html\n",
    "\n",
    "7. From inspecting the html, the information of interest is contained within a \"div\" of class_=\"downloads\". \n",
    "We create a container object referencing the this \"div\"\n",
    "\n",
    "8. Then find the first <a> tag which corresponds to the sample image link and extract the href attribute which contains\n",
    "the link to the fullsized image\n",
    "\n",
    "9. We then create a dictionary with \"title\" and \"img_url\" keys that match with our hem_title and img_url values respectively.\n",
    "\n",
    "10. Lastly we append the dictionary to our hemisphere_image_info list\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "for div in soup_div_list:\n",
    "    \n",
    "# using the .visit() method tell the browser to visit the url\n",
    "    browser.visit(hem_imgs_url)\n",
    "\n",
    "#sleep code to allow browser page to load\n",
    "    time.sleep(10)\n",
    "    hem_title = div.find(\"h3\").text\n",
    "    try:\n",
    "        browser.click_link_by_partial_text(hem_title)\n",
    "        nxt_pg_html = browser.html\n",
    "        \n",
    "        nxt_pg_soup = BeautifulSoup(nxt_pg_html, 'html.parser')\n",
    "\n",
    "# Create a soup object for the \"div\" thats contains the content of interest.. in this case the <a> tags with image links\n",
    "        container_div = nxt_pg_soup.find(\"div\", class_=\"downloads\")\n",
    "\n",
    "# Find the first <a> tag which corresponds to the sample image link and extract the href attribute\n",
    "        img_url = container_div.a[\"href\"]\n",
    "        img_title_dict = {\"title\": hem_title, \"img_url\": img_url }\n",
    "        \n",
    "# Append the dictionary to our hemisphere_image_info list\n",
    "        hemisphere_image_info.append(img_title_dict)\n",
    "    except:\n",
    "        print(\"Scraping Complete\")\n",
    "\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Cerberus Hemisphere Enhanced',\n",
       "  'img_url': 'http://astropedia.astrogeology.usgs.gov/download/Mars/Viking/cerberus_enhanced.tif/full.jpg'},\n",
       " {'title': 'Schiaparelli Hemisphere Enhanced',\n",
       "  'img_url': 'http://astropedia.astrogeology.usgs.gov/download/Mars/Viking/schiaparelli_enhanced.tif/full.jpg'},\n",
       " {'title': 'Syrtis Major Hemisphere Enhanced',\n",
       "  'img_url': 'http://astropedia.astrogeology.usgs.gov/download/Mars/Viking/syrtis_major_enhanced.tif/full.jpg'},\n",
       " {'title': 'Valles Marineris Hemisphere Enhanced',\n",
       "  'img_url': 'http://astropedia.astrogeology.usgs.gov/download/Mars/Viking/valles_marineris_enhanced.tif/full.jpg'}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hemisphere_image_info"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
